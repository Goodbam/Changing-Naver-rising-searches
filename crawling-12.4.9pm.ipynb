{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawling(everyone).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Goodbam/Changing-Naver-rising-searches/blob/master/crawling(everyone).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tssl247HkHCi",
        "colab_type": "code",
        "outputId": "3b5e5b8f-4309-4591-fd86-bd1afc674ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install lxml\n",
        "!pip install parse"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lxml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 6.5MB/s \n",
            "\u001b[?25hInstalling collected packages: lxml\n",
            "Successfully installed lxml-4.2.5\n",
            "Collecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/98/809e53e5778c59c4af9eb920605e7a8ab439407efbe89a6d51a46efd1937/parse-1.9.0.tar.gz\n",
            "Building wheels for collected packages: parse\n",
            "  Running setup.py bdist_wheel for parse ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ef/db/c6/18568a2cc574848f3996ac4552241fbec046b7be29feb2077d\n",
            "Successfully built parse\n",
            "Installing collected packages: parse\n",
            "Successfully installed parse-1.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uxKEDAv_kLKH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests                 # url request\n",
        "from lxml.html import parse     # parsing the data as the html form\n",
        "from io import StringIO         # String I/O module\n",
        "\n",
        "THRESHOLD = 25 #limit to check whether they are relative or not"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VnRDApPylNT_",
        "colab_type": "code",
        "outputId": "20908bfe-8a24-4b46-9cd0-c972768b44db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "cell_type": "code",
      "source": [
        "url = \"https://www.naver.com\"\n",
        "\n",
        "# Get html source\n",
        "text = requests.get(url).text\n",
        "\n",
        "# parsing the data as the html form\n",
        "text_source = StringIO(text)    # read the data as string\n",
        "parsed = parse(text_source)     # source ->parsing the data as the html form \n",
        "\n",
        "#find root node\n",
        "doc = parsed.getroot()\n",
        "\n",
        "# doc.findall(\".//TagName\") => find the tag lists\n",
        "uls = doc.findall('.//ul')\n",
        "\n",
        "#Get the Rising searches\n",
        "ul = uls[10]   \n",
        "spans = ul.findall('.//span')\n",
        "\n",
        "data_list= []  # to save all the keywords\n",
        "\n",
        "cnt = 1\n",
        "for data in spans:\n",
        "  if cnt % 2 == 0:  \n",
        "    print(int((cnt+1)/2),'->',data.text_content().strip())  # show the existing rising searches\n",
        "  cnt +=1  \n",
        "  data_list.append(data.text_content().strip()) #save each keyword\n",
        "    \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 -> sbs 드라마\n",
            "2 -> 나쁜형사\n",
            "3 -> 이커머스\n",
            "4 -> 2019 수능 등급컷\n",
            "5 -> 이선정\n",
            "6 -> 이설\n",
            "7 -> 수능 등급컷\n",
            "8 -> 홍영표\n",
            "9 -> 배지현\n",
            "10 -> 캡틴 마블\n",
            "11 -> 붐붐파워\n",
            "12 -> 진학사\n",
            "13 -> 수능 성적 발표\n",
            "14 -> 노니\n",
            "15 -> 이일재\n",
            "16 -> 황지만\n",
            "17 -> 노니 쇳가루\n",
            "18 -> 정재형\n",
            "19 -> 식품안전나라\n",
            "20 -> 광주형 일자리\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Xk_oFVcFzQU",
        "colab_type": "code",
        "outputId": "df412085-bb17-490f-a912-5dbcb0199b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "path='https://search.naver.com/search.naver?sm=top_hty&fbm=1&ie=utf8&query=' #This is the url for surfing\n",
        "\n",
        "matrix = [] #to count the score of relationship\n",
        "label = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20] #disjoint set\n",
        "\n",
        "cnt1 = 0\n",
        "\n",
        "for i in range(0, 20):\n",
        "  word = data_list[1 + cnt1]\n",
        "  search = path + word #url searching the keyword\n",
        "  text = requests.get(search).text # no worry about encoding url\n",
        "  \n",
        "  row = []\n",
        "  cnt2 = 0\n",
        "  for j in range(0, 20):\n",
        "    if cnt1 != cnt2: #no need to check when they are equivalent\n",
        "      target = data_list[1 + cnt2];\n",
        "      row.append(text.count(target) + 7*word.count(target)) \n",
        " \n",
        "      if row[int(cnt2/2)] >= THRESHOLD: #IF they are relative,\n",
        "        print(word, ' and ', target,)\n",
        "        #the representive number should be minimized\n",
        "        \n",
        "        #여기 정규화를 해서 row의합으로 각 row의 값을 나누던지 해보자\n",
        "        #그러려니, 전체적으로 점수 자체가 높은 것은 감안해서 평균을 다시 루트씌우고 곱하든지 고려해보자\n",
        "        #A와 B 관련 , B C 관련 이면 A C는 관련이 있는건가? 이것도 감안\n",
        "        \n",
        "        if label[int(cnt1/2)] < label[int(cnt2/2)]:\n",
        "          label[int(cnt2/2)] = label[int(cnt1/2)]  \n",
        "        \n",
        "        else:\n",
        "          label[int(cnt1/2)] = label[int(cnt2/2)]\n",
        "    \n",
        "    else :\n",
        "      row.append(0)\n",
        "    \n",
        "    cnt2 += 2\n",
        "  \n",
        "  matrix.append(row)  \n",
        "  cnt1 += 2\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "나쁜형사  and  이설\n",
            "2019 수능 등급컷  and  수능 등급컷\n",
            "이설  and  나쁜형사\n",
            "수능 등급컷  and  2019 수능 등급컷\n",
            "진학사  and  2019 수능 등급컷\n",
            "진학사  and  수능 등급컷\n",
            "수능 성적 발표  and  수능 등급컷\n",
            "황지만  and  이선정\n",
            "노니 쇳가루  and  노니\n",
            "식품안전나라  and  노니\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G4W-z5rSF0Gz",
        "colab_type": "code",
        "outputId": "a967e2cd-ae92-4487-e868-ec63a22f6325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "print('\\n',label,'\\n')\n",
        "\n",
        "print(matrix)\n",
        "\n",
        "score =[0]\n",
        "score = score*20;\n",
        "\n",
        "for i in range(0,20):\n",
        "  score[label[i] - 1] += 1/(i+1); #label is 1~20 => needed to subtract 1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " [1, 2, 3, 4, 5, 2, 4, 8, 9, 10, 11, 4, 4, 14, 15, 5, 14, 18, 14, 20] \n",
            "\n",
            "[[0, 11, 9, 11, 9, 9, 20, 9, 9, 9, 9, 9, 11, 20, 11, 9, 11, 9, 9, 9], [11, 0, 9, 11, 9, 26, 20, 9, 9, 9, 9, 9, 11, 20, 11, 9, 11, 9, 9, 9], [11, 9, 0, 11, 9, 9, 20, 9, 9, 9, 9, 9, 11, 20, 11, 9, 11, 9, 9, 9], [11, 9, 9, 0, 9, 9, 115, 9, 9, 9, 9, 23, 14, 20, 11, 9, 11, 9, 9, 9], [11, 9, 9, 11, 0, 9, 20, 9, 9, 9, 9, 9, 11, 20, 11, 10, 11, 9, 9, 9], [11, 108, 9, 11, 9, 0, 20, 9, 9, 9, 9, 9, 11, 20, 11, 9, 11, 9, 9, 9], [11, 9, 9, 35, 9, 9, 0, 9, 9, 9, 9, 14, 15, 20, 11, 9, 11, 9, 9, 9], [11, 9, 9, 11, 9, 9, 20, 0, 9, 9, 9, 9, 11, 20, 11, 9, 11, 9, 9, 9], [11, 9, 9, 11, 9, 9, 20, 9, 0, 9, 9, 9, 11, 20, 11, 9, 11, 9, 9, 9], [11, 9, 9, 11, 9, 9, 20, 9, 9, 0, 9, 9, 11, 20, 11, 9, 11, 9, 9, 9], [11, 9, 9, 11, 9, 9, 20, 9, 9, 9, 0, 9, 11, 20, 11, 9, 11, 9, 9, 9], [11, 9, 9, 31, 9, 9, 43, 9, 9, 9, 9, 0, 14, 20, 11, 9, 11, 9, 9, 9], [11, 9, 9, 19, 9, 9, 33, 9, 9, 9, 9, 17, 0, 20, 11, 9, 11, 9, 9, 9], [11, 9, 9, 11, 9, 9, 20, 9, 9, 9, 9, 9, 11, 0, 11, 9, 21, 9, 11, 9], [11, 9, 9, 11, 9, 9, 20, 9, 9, 9, 9, 9, 11, 20, 0, 9, 11, 9, 9, 9], [11, 9, 9, 11, 49, 9, 20, 9, 9, 9, 9, 9, 11, 20, 11, 0, 11, 9, 9, 0], [11, 9, 9, 11, 9, 9, 20, 9, 9, 9, 9, 9, 11, 214, 11, 9, 0, 9, 17, 0], [11, 9, 9, 11, 9, 9, 20, 9, 9, 9, 9, 9, 11, 20, 11, 9, 11, 0, 9, 0], [11, 9, 9, 11, 9, 9, 20, 9, 9, 9, 9, 9, 11, 46, 11, 9, 14, 9, 0, 0], [6, 4, 4, 6, 4, 4, 10, 4, 4, 4, 4, 4, 6, 10, 6, 4, 6, 4, 4, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QUTNRYleF0KA",
        "colab_type": "code",
        "outputId": "45fa898f-d572-4e6c-bf90-4aa34b77a70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "#RESULT\n",
        "print('\\n☆급상승 검색어 순위☆\\n')\n",
        "cnt = 1;\n",
        "\n",
        "#sorting according to SCORE\n",
        "for i in range(0,20):\n",
        "  max = 0\n",
        "  for j in range(1,20):\n",
        "    if score[j] > score[max]:\n",
        "      max = j;\n",
        "  \n",
        "  relative_word = ''\n",
        "  \n",
        "  for j in range(0,20):\n",
        "    if label[j] == max + 1:\n",
        "      relative_word += data_list[1 + 2*j] + ' | ' \n",
        "  \n",
        "  score[max] = 0;\n",
        "  \n",
        "  print(cnt, '. ', relative_word[0:-3])\n",
        "  cnt += 1\n",
        "  if(score.count(0) == 20):\n",
        "    break  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "☆급상승 검색어 순위☆\n",
            "\n",
            "1 .  sbs 드라마\n",
            "2 .  나쁜형사 | 이설\n",
            "3 .  2019 수능 등급컷 | 수능 등급컷 | 진학사 | 수능 성적 발표\n",
            "4 .  이커머스\n",
            "5 .  이선정 | 황지만\n",
            "6 .  노니 | 노니 쇳가루 | 식품안전나라\n",
            "7 .  홍영표\n",
            "8 .  배지현\n",
            "9 .  캡틴 마블\n",
            "10 .  붐붐파워\n",
            "11 .  이일재\n",
            "12 .  정재형\n",
            "13 .  광주형 일자리\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Se8kjTi5qAY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
